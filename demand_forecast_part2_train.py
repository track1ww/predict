# -*- coding: utf-8 -*-
"""
1ì›” 2ì¼ ë°œì£¼ìš© ìˆ˜ìš”ì˜ˆì¸¡ - Part 2: LightGBM ëª¨ë¸ í•™ìŠµ & WMAPE ê²€ì¦
[ì—ëŸ¬ ìˆ˜ì • ì´ë ¥]
- bin size 3255: CATEGORICAL_FEATURESì—ì„œ sku_name ì œê±°, Dataset max_bin=255 ëª…ì‹œ
- left_count > 0: min_child_samples ë„ˆë¬´ ì‘ì„ ë•Œ GPUì—ì„œ ë…¸ë“œ ë¶„í•  ì‹¤íŒ¨
  â†’ min_child_samples=200, max_depth=6ìœ¼ë¡œ ì•ˆì •í™” (num_leaves=127 ìœ ì§€)
- ê³¼ì†Œì˜ˆì¸¡: tweedie_variance_power=1.0, learning_rate=0.02, early_stopping=500
- ì¹´í…Œê³ ë¦¬ë³„ í¸í–¥ ë³´ì • ê³„ìˆ˜(bias_corr) â†’ Part 3 ì˜ˆì¸¡ê°’ í›„ì²˜ë¦¬ì— ì‚¬ìš©
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
import warnings
warnings.filterwarnings('ignore')

from demand_forecast_part1_features import (
    load_data, build_features, encode_and_clean,
    split_data, build_forecast_rows,
    wmape, FEATURE_COLS, TARGET_COL
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. LightGBM íŒŒë¼ë¯¸í„°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LGB_PARAMS = {
    # 'device'              : 'gpu',   # GPU: left_count ì—ëŸ¬ ë°˜ë³µ â†’ CPU ì‚¬ìš©
    'objective'              : 'tweedie',
    'tweedie_variance_power' : 1.0,
    'metric'                 : 'rmse',
    'verbosity'              : -1,
    'random_state'           : 42,
    'n_jobs'                 : -1,       # CPU ì „ì²´ ì½”ì–´ ì‚¬ìš©
    # âœ… GPU bin ì œí•œ
    'max_bin'                : 255,
    # âœ… íŠ¸ë¦¬ êµ¬ì¡° â€” GPU ì•ˆì •ì„± ìš°ì„ 
    'num_leaves'             : 127,     # CPUì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥
    'max_depth'              : 6,       # 8â†’6: ë…¸ë“œ ë¶„í•  ì‹¤íŒ¨ ë°©ì§€
    'min_child_samples'      : 20,      # CPU ê¸°ë³¸ê°’
    'min_child_weight'       : 1e-3,
    # í•™ìŠµ
    'learning_rate'          : 0.02,
    'n_estimators'           : 10000,  # ì¶©ë¶„í•œ í•™ìŠµ ë¼ìš´ë“œ í™•ë³´
    'subsample'              : 0.85,
    'subsample_freq'         : 1,
    'colsample_bytree'       : 0.85,
    'reg_alpha'              : 0.05,
    'reg_lambda'             : 0.05,
    'cat_smooth'             : 10,
}

# âœ… sku_name ì œê±°: ê³ ìœ ê°’ 3,300ê°œ â†’ bin 3,255 ìƒì„± â†’ GPU ì—ëŸ¬
CATEGORICAL_FEATURES = ['warehouse', 'm_cat', 'season_name']


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. ëª¨ë¸ í•™ìŠµ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def train_model(train: pd.DataFrame, valid: pd.DataFrame):
    print("=" * 60)
    print("ğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

    X_train = train[FEATURE_COLS]
    y_train = train[TARGET_COL]
    X_valid = valid[FEATURE_COLS]
    y_valid = valid[TARGET_COL]

    # ì‹œê°„ ê°€ì¤‘ì¹˜: ìµœê·¼ 60ì¼ ê°•ì¡°
    max_date = train['sales_date'].max()
    days_from_max = (max_date - train['sales_date']).dt.days
    sample_weight = np.exp(-days_from_max / 45)   # ì „ì²´ í•™ìŠµ ê¸°ì¤€ ìµœê·¼ 45ì¼ ê°•ì¡°

    # âœ… Datasetì—ë„ max_bin=255 ëª…ì‹œ (paramsì™€ ë…ë¦½ì ìœ¼ë¡œ bin ìƒì„±ë¨)
    dtrain = lgb.Dataset(
        X_train, label=y_train,
        weight=sample_weight,
        categorical_feature=CATEGORICAL_FEATURES,
        free_raw_data=False,
        params={'max_bin': 255}
    )
    dvalid = lgb.Dataset(
        X_valid, label=y_valid,
        reference=dtrain,
        categorical_feature=CATEGORICAL_FEATURES,
        free_raw_data=False,
        params={'max_bin': 255}
    )

    wmape_log = []

    def wmape_callback(env):
        if env.iteration % 100 == 0:
            pred = np.maximum(env.model.predict(X_valid), 0)
            wm   = wmape(y_valid.values, pred)
            wmape_log.append((env.iteration, wm))
            print(f"  [Round {env.iteration:4d}] WMAPE: {wm:.2f}%")

    callbacks = [
        lgb.early_stopping(stopping_rounds=500, verbose=False),
        lgb.log_evaluation(period=-1),
        wmape_callback,
    ]

    model = lgb.train(
        params=LGB_PARAMS,
        train_set=dtrain,
        valid_sets=[dvalid],
        callbacks=callbacks,
    )

    print(f"\n  âœ… í•™ìŠµ ì™„ë£Œ | Best iteration: {model.best_iteration}")
    return model, wmape_log


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ê²€ì¦ í‰ê°€ + ì¹´í…Œê³ ë¦¬ë³„ í¸í–¥ ë³´ì • ê³„ìˆ˜ ì‚°ì¶œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def evaluate(model, valid: pd.DataFrame):
    print("\n" + "=" * 60)
    print("ğŸ“Š ê²€ì¦ í‰ê°€ (WMAPE)")

    preds = np.maximum(model.predict(valid[FEATURE_COLS]), 0)

    valid = valid.copy()
    valid['pred']    = preds
    valid['abs_err'] = np.abs(valid['qty'] - valid['pred'])

    # ì „ì²´ WMAPE
    total_wm = wmape(valid['qty'].values, valid['pred'].values)
    print(f"\n  ğŸ¯ ì „ì²´ WMAPE      : {total_wm:.2f}%  (ëª©í‘œ <= 10%)")
    print(f"  {'âœ… ëª©í‘œ ë‹¬ì„±!' if total_wm <= 10 else 'âš ï¸  ì¶”ê°€ íŠœë‹ í•„ìš”'}")

    # ì¹´í…Œê³ ë¦¬ë³„ WMAPE
    grp = valid.groupby('m_cat').apply(
        lambda g: wmape(g['qty'].values, g['pred'].values)
    ).reset_index()
    grp.columns = ['m_cat', 'wmape']
    print(f"\n  ğŸ“‹ ì§‘ê³„ë³„ WMAPE (ì½”ë“œê°’ ê¸°ì¤€, Part 3ì—ì„œ ì›ë³¸ëª… ë³µì›):")

    # í¸í–¥ ë³´ì • ê³„ìˆ˜: ì‹¤ì œí•© / ì˜ˆì¸¡í•© (ì¹´í…Œê³ ë¦¬ë³„ ì²´ê³„ì  ê³¼ì†Œì˜ˆì¸¡ ìˆ˜ì¹˜ ë³´ì •)
    bias_corr = valid.groupby('m_cat').apply(
        lambda g: g['qty'].sum() / (g['pred'].sum() + 1e-9)
    ).reset_index()
    bias_corr.columns = ['m_cat', 'bias_correction']

    valid_corr = valid.merge(bias_corr, on='m_cat', how='left')
    valid_corr['pred_corrected'] = valid_corr['pred'] * valid_corr['bias_correction']
    corrected_wm = wmape(valid_corr['qty'].values, valid_corr['pred_corrected'].values)
    print(f"  ğŸ”§ í¸í–¥ ë³´ì • í›„ WMAPE : {corrected_wm:.2f}%")

    return valid, total_wm, bias_corr


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. í”¼ì²˜ ì¤‘ìš”ë„ ì¶œë ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def print_feature_importance(model, top_n: int = 20):
    imp_df = pd.DataFrame({
        'feature'   : model.feature_name(),
        'importance': model.feature_importance(importance_type='gain'),
    }).sort_values('importance', ascending=False).head(top_n)

    print(f"\nğŸ” í”¼ì²˜ ì¤‘ìš”ë„ Top {top_n}:")
    print("-" * 45)
    for _, row in imp_df.iterrows():
        bar = "â–ˆ" * int(row['importance'] / imp_df['importance'].max() * 30)
        print(f"  {row['feature']:25s} {bar}")
    print("-" * 45)
    return imp_df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ëª¨ë¸ ì €ì¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def save_model(model, path: str = 'lgb_demand_model.pkl'):
    joblib.dump(model, path)
    model.save_model(path.replace('.pkl', '.txt'))
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥: {path}, {path.replace('.pkl', '.txt')}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == '__main__':
    df_raw  = load_data('sales_data.parquet')
    df_feat = build_features(df_raw.copy())
    df_enc  = encode_and_clean(df_feat)
    train, valid = split_data(df_enc)

    model, wmape_log = train_model(train, valid)
    valid_result, total_wmape, bias_corr = evaluate(model, valid)
    feat_imp = print_feature_importance(model, top_n=20)

    joblib.dump(bias_corr, 'bias_correction.pkl')
    print(f"  ğŸ’¾ í¸í–¥ ë³´ì • ê³„ìˆ˜ ì €ì¥: bias_correction.pkl")
    save_model(model)

    print("\nâœ… Part 2 ì™„ë£Œ")
    print(f"   ìµœì¢… ê²€ì¦ WMAPE: {total_wmape:.2f}%")