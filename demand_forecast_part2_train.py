# -*- coding: utf-8 -*-
"""
1ì›” 2ì¼ ë°œì£¼ìš© ìˆ˜ìš”ì˜ˆì¸¡ - Part 2: LightGBM ëª¨ë¸ í•™ìŠµ & WMAPE ê²€ì¦
[ì—ëŸ¬ ìˆ˜ì • ì´ë ¥]
- bin size 3255: CATEGORICAL_FEATURESì—ì„œ sku_name ì œê±°, Dataset max_bin=255 ëª…ì‹œ
- left_count > 0: min_child_samples ë„ˆë¬´ ì‘ì„ ë•Œ GPUì—ì„œ ë…¸ë“œ ë¶„í•  ì‹¤íŒ¨
  â†’ min_child_samples=200, max_depth=6ìœ¼ë¡œ ì•ˆì •í™” (num_leaves=127 ìœ ì§€)
- ê³¼ì†Œì˜ˆì¸¡: tweedie_variance_power=1.0, learning_rate=0.02, early_stopping=500
- ì¹´í…Œê³ ë¦¬ë³„ í¸í–¥ ë³´ì • ê³„ìˆ˜(bias_corr) â†’ Part 3 ì˜ˆì¸¡ê°’ í›„ì²˜ë¦¬ì— ì‚¬ìš©
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
import warnings
warnings.filterwarnings('ignore')

from demand_forecast_part1_features import (
    load_data, build_features, encode_and_clean,
    split_data, build_forecast_rows,
    wmape, FEATURE_COLS, TARGET_COL
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. LightGBM íŒŒë¼ë¯¸í„°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LGB_PARAMS = {
    # 'device'              : 'gpu',   # GPU: left_count ì—ëŸ¬ ë°˜ë³µ â†’ CPU ì‚¬ìš©
    'objective'              : 'tweedie',
    'tweedie_variance_power' : 1.0,
    'metric'                 : 'rmse',
    'verbosity'              : -1,
    'random_state'           : 42,
    'n_jobs'                 : -1,       # CPU ì „ì²´ ì½”ì–´ ì‚¬ìš©
    # âœ… GPU bin ì œí•œ
    'max_bin'                : 255,
    # âœ… íŠ¸ë¦¬ êµ¬ì¡° â€” GPU ì•ˆì •ì„± ìš°ì„ 
    'num_leaves'             : 127,     # CPUì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥
    'max_depth'              : 6,       # 8â†’6: ë…¸ë“œ ë¶„í•  ì‹¤íŒ¨ ë°©ì§€
    'min_child_samples'      : 20,      # CPU ê¸°ë³¸ê°’
    'min_child_weight'       : 1e-3,
    # í•™ìŠµ
    'learning_rate'          : 0.02,
    'n_estimators'           : 10000,  # ì¶©ë¶„í•œ í•™ìŠµ ë¼ìš´ë“œ í™•ë³´
    'subsample'              : 0.85,
    'subsample_freq'         : 1,
    'colsample_bytree'       : 0.85,
    'reg_alpha'              : 0.05,
    'reg_lambda'             : 0.05,
    'cat_smooth'             : 10,
}

# âœ… sku_name ì œê±°: ê³ ìœ ê°’ 3,300ê°œ â†’ bin 3,255 ìƒì„± â†’ GPU ì—ëŸ¬
CATEGORICAL_FEATURES = ['warehouse', 'm_cat', 'season_name']


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def run_optuna(train: pd.DataFrame, valid: pd.DataFrame,
               n_trials: int = 50) -> dict:
    """
    Optunaë¡œ LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    ëª©ì í•¨ìˆ˜: ì—°íœ´ ì§í›„ ê²€ì¦ì…‹ WMAPE ìµœì†Œí™”
    """
    import optuna
    optuna.logging.set_verbosity(optuna.logging.WARNING)

    X_train = train[FEATURE_COLS]
    y_train = train[TARGET_COL]
    X_valid = valid[FEATURE_COLS]
    y_valid = valid[TARGET_COL]

    max_date = train['sales_date'].max()
    days_from_max = (max_date - train['sales_date']).dt.days
    sample_weight = np.exp(-days_from_max / 45)

    def objective(trial):
        params = {
            # ê³ ì •
            'objective'              : 'tweedie',
            'metric'                 : 'rmse',
            'verbosity'              : -1,
            'random_state'           : 42,
            'n_jobs'                 : -1,
            'max_bin'                : 255,
            # íƒìƒ‰ (ì´ì „ best ê·¼ë°© ì •ë°€ íƒìƒ‰)
            # best: num_leaves=221, lr=0.049, max_depth=10, min_child=110
            'tweedie_variance_power' : trial.suggest_float('tweedie_variance_power', 1.0, 1.5),
            'num_leaves'             : trial.suggest_int('num_leaves', 63, 255),
            'max_depth'              : trial.suggest_int('max_depth', 6, 12),
            'min_child_samples'      : trial.suggest_int('min_child_samples', 20, 200),
            'learning_rate'          : trial.suggest_float('learning_rate', 0.02, 0.1, log=True),
            'subsample'              : trial.suggest_float('subsample', 0.6, 0.85),
            'colsample_bytree'       : trial.suggest_float('colsample_bytree', 0.65, 0.85),
            'reg_alpha'              : trial.suggest_float('reg_alpha', 1e-3, 0.1, log=True),
            'reg_lambda'             : trial.suggest_float('reg_lambda', 0.3, 1.0, log=True),
            'cat_smooth'             : trial.suggest_int('cat_smooth', 12, 20),
        }

        dtrain = lgb.Dataset(
            X_train, label=y_train,
            weight=sample_weight,
            categorical_feature=CATEGORICAL_FEATURES,
            free_raw_data=False,
            params={'max_bin': 255}
        )
        dvalid = lgb.Dataset(
            X_valid, label=y_valid,
            reference=dtrain,
            categorical_feature=CATEGORICAL_FEATURES,
            free_raw_data=False,
            params={'max_bin': 255}
        )

        callbacks = [
            lgb.early_stopping(stopping_rounds=200, verbose=False),
            lgb.log_evaluation(period=-1),
        ]

        model = lgb.train(
            params=params,
            train_set=dtrain,
            num_boost_round=5000,
            valid_sets=[dvalid],
            callbacks=callbacks,
        )

        preds = np.maximum(model.predict(X_valid), 0)
        return wmape(y_valid.values, preds)

    print(f"\nğŸ” Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì‹œì‘ ({n_trials} trials)...")
    study = optuna.create_study(direction='minimize',
                                 sampler=optuna.samplers.TPESampler(seed=42))
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

    best = study.best_params
    best_wmape = study.best_value
    print(f"\n  âœ… Optuna ì™„ë£Œ | Best WMAPE: {best_wmape:.2f}%")
    print(f"  ğŸ“‹ Best params:")
    for k, v in best.items():
        print(f"     {k}: {v}")

    return best, best_wmape


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ëª¨ë¸ í•™ìŠµ (Optuna ê²°ê³¼ ë˜ëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def train_model(train: pd.DataFrame, valid: pd.DataFrame,
                best_params: dict = None):
    print("=" * 60)
    print("ğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

    X_train = train[FEATURE_COLS]
    y_train = train[TARGET_COL]
    X_valid = valid[FEATURE_COLS]
    y_valid = valid[TARGET_COL]

    # Optuna ê²°ê³¼ íŒŒë¼ë¯¸í„° ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
    if best_params is not None:
        params = {
            'objective'              : 'tweedie',
            'metric'                 : 'rmse',
            'verbosity'              : -1,
            'random_state'           : 42,
            'n_jobs'                 : -1,
            'max_bin'                : 255,
            **best_params,
        }
        print(f"  âœ… Optuna ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©")
    else:
        params = LGB_PARAMS
        print(f"  â„¹ï¸  ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")

    # ì‹œê°„ ê°€ì¤‘ì¹˜: ìµœê·¼ 45ì¼ ê°•ì¡°
    max_date = train['sales_date'].max()
    days_from_max = (max_date - train['sales_date']).dt.days
    sample_weight = np.exp(-days_from_max / 45)   # ì „ì²´ í•™ìŠµ ê¸°ì¤€ ìµœê·¼ 45ì¼ ê°•ì¡°

    # âœ… Datasetì—ë„ max_bin=255 ëª…ì‹œ (paramsì™€ ë…ë¦½ì ìœ¼ë¡œ bin ìƒì„±ë¨)
    dtrain = lgb.Dataset(
        X_train, label=y_train,
        weight=sample_weight,
        categorical_feature=CATEGORICAL_FEATURES,
        free_raw_data=False,
        params={'max_bin': 255}
    )
    dvalid = lgb.Dataset(
        X_valid, label=y_valid,
        reference=dtrain,
        categorical_feature=CATEGORICAL_FEATURES,
        free_raw_data=False,
        params={'max_bin': 255}
    )

    wmape_log = []

    def wmape_callback(env):
        if env.iteration % 100 == 0:
            pred = np.maximum(env.model.predict(X_valid), 0)
            wm   = wmape(y_valid.values, pred)
            wmape_log.append((env.iteration, wm))
            print(f"  [Round {env.iteration:4d}] WMAPE: {wm:.2f}%")

    callbacks = [
        lgb.early_stopping(stopping_rounds=500, verbose=False),
        lgb.log_evaluation(period=-1),
        wmape_callback,
    ]

    model = lgb.train(
        params=params,
        train_set=dtrain,
        num_boost_round=10000,
        valid_sets=[dvalid],
        callbacks=callbacks,
    )

    print(f"\n  âœ… í•™ìŠµ ì™„ë£Œ | Best iteration: {model.best_iteration}")
    return model, wmape_log


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ê²€ì¦ í‰ê°€ + ì¹´í…Œê³ ë¦¬ë³„ í¸í–¥ ë³´ì • ê³„ìˆ˜ ì‚°ì¶œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def evaluate(model, valid: pd.DataFrame):
    print("\n" + "=" * 60)
    print("ğŸ“Š ê²€ì¦ í‰ê°€ (WMAPE)")

    preds = np.maximum(model.predict(valid[FEATURE_COLS]), 0)

    valid = valid.copy()
    valid['pred']    = preds
    valid['abs_err'] = np.abs(valid['qty'] - valid['pred'])

    # ì „ì²´ WMAPE
    total_wm = wmape(valid['qty'].values, valid['pred'].values)
    print(f"\n  ğŸ¯ ì „ì²´ WMAPE      : {total_wm:.2f}%  (ëª©í‘œ <= 10%)")
    print(f"  {'âœ… ëª©í‘œ ë‹¬ì„±!' if total_wm <= 10 else 'âš ï¸  ì¶”ê°€ íŠœë‹ í•„ìš”'}")

    # ì¹´í…Œê³ ë¦¬ë³„ WMAPE
    grp = valid.groupby('m_cat').apply(
        lambda g: wmape(g['qty'].values, g['pred'].values)
    ).reset_index()
    grp.columns = ['m_cat', 'wmape']
    print(f"\n  ğŸ“‹ ì§‘ê³„ë³„ WMAPE (ì½”ë“œê°’ ê¸°ì¤€, Part 3ì—ì„œ ì›ë³¸ëª… ë³µì›):")

    # í¸í–¥ ë³´ì • ê³„ìˆ˜: ì‹¤ì œí•© / ì˜ˆì¸¡í•© (ì¹´í…Œê³ ë¦¬ë³„ ì²´ê³„ì  ê³¼ì†Œì˜ˆì¸¡ ìˆ˜ì¹˜ ë³´ì •)
    bias_corr = valid.groupby('m_cat').apply(
        lambda g: g['qty'].sum() / (g['pred'].sum() + 1e-9)
    ).reset_index()
    bias_corr.columns = ['m_cat', 'bias_correction']

    valid_corr = valid.merge(bias_corr, on='m_cat', how='left')
    valid_corr['pred_corrected'] = valid_corr['pred'] * valid_corr['bias_correction']
    corrected_wm = wmape(valid_corr['qty'].values, valid_corr['pred_corrected'].values)
    print(f"  ğŸ”§ í¸í–¥ ë³´ì • í›„ WMAPE : {corrected_wm:.2f}%")

    return valid, total_wm, bias_corr


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. í”¼ì²˜ ì¤‘ìš”ë„ ì¶œë ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def print_feature_importance(model, top_n: int = 20):
    imp_df = pd.DataFrame({
        'feature'   : model.feature_name(),
        'importance': model.feature_importance(importance_type='gain'),
    }).sort_values('importance', ascending=False).head(top_n)

    print(f"\nğŸ” í”¼ì²˜ ì¤‘ìš”ë„ Top {top_n}:")
    print("-" * 45)
    for _, row in imp_df.iterrows():
        bar = "â–ˆ" * int(row['importance'] / imp_df['importance'].max() * 30)
        print(f"  {row['feature']:25s} {bar}")
    print("-" * 45)
    return imp_df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ëª¨ë¸ ì €ì¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def save_model(model, path: str = 'lgb_demand_model.pkl'):
    joblib.dump(model, path)
    model.save_model(path.replace('.pkl', '.txt'))
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥: {path}, {path.replace('.pkl', '.txt')}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == '__main__':
    df_raw  = load_data('sales_data.parquet')
    df_feat = build_features(df_raw.copy())
    df_enc  = encode_and_clean(df_feat)
    train, valid = split_data(df_enc)

    # Optuna ì‹¤í–‰ (n_trials ì¡°ì • ê°€ëŠ¥, 1trialë‹¹ ì•½ 20~30ì´ˆ)
    best_params, best_wmape = run_optuna(train, valid, n_trials=50)

    model, wmape_log = train_model(train, valid, best_params)
    valid_result, total_wmape, bias_corr = evaluate(model, valid)
    feat_imp = print_feature_importance(model, top_n=20)

    joblib.dump(bias_corr, 'bias_correction.pkl')
    print(f"  ğŸ’¾ í¸í–¥ ë³´ì • ê³„ìˆ˜ ì €ì¥: bias_correction.pkl")
    save_model(model)

    print("\nâœ… Part 2 ì™„ë£Œ")
    print(f"   ìµœì¢… ê²€ì¦ WMAPE: {total_wmape:.2f}%")